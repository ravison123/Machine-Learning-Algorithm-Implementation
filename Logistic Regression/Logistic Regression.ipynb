{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:56.712163Z",
     "start_time": "2021-02-18T14:00:44.878896Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:56.737707Z",
     "start_time": "2021-02-18T14:00:56.716617Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare x matrix by adding dummy feature (x0 = 1) into it\n",
    "def prepare_matrices(x, y):\n",
    "    '''\n",
    "    This function prepares x matrix (by adding a dummy feature, x0) and confirms whether shapes of x and y matrix match.\n",
    "    Dummy feature: Feature whose value is 1\n",
    "    \n",
    "    Arguements:\n",
    "    x: Feature Matrix (numpy array of shape (m, n), m = number of examples, n = number of features)\n",
    "    y: Output Vector / Target Vector (numpy array of shape (m, 1))\n",
    "    \n",
    "    Returns:\n",
    "    x: Feature Matrix (numpy array of shape (m, n+1)\n",
    "    y = Output Vector / Target Vector (numpy array of shape (m, 1))\n",
    "    '''\n",
    "    m = x.shape[0]\n",
    "    n = x.shape[1]\n",
    "    x_0 = np.ones(shape = (m, 1))\n",
    "    x_1 = x.reshape((m, n))\n",
    "    x = np.concatenate((x_0, x_1), axis = 1)\n",
    "    y = y.reshape((m, 1))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:56.810821Z",
     "start_time": "2021-02-18T14:00:56.743392Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_parameter_matrix(n, method = 'zeros'):\n",
    "    '''\n",
    "    This function initializes parameter matrix.\n",
    "    \n",
    "    Arguements:\n",
    "    n: Number of features (This include dummy featue x0)\n",
    "    method: zeros / random. The method used to initialize paramter matrix.\n",
    "    (default: 'zeros', i.e. initialization with zeros)\n",
    "    \n",
    "    Returns:\n",
    "    theta: Parameter matrix of shape (n, 1), n = number of features \n",
    "    '''\n",
    "    if method == 'zeros':\n",
    "        theta = np.zeros(shape = (n, 1))\n",
    "    elif method == 'random':\n",
    "        theta = np.random.rand(n, 1)\n",
    "    assert(theta.shape == (n, 1))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:56.871336Z",
     "start_time": "2021-02-18T14:00:56.816646Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(arr):\n",
    "    '''\n",
    "    This function calculates sigmoid function of every element of array and return a new array.\n",
    "    \n",
    "    Arguements:\n",
    "    arr: Array of shape (m, 1)\n",
    "    \n",
    "    Returns:\n",
    "    sigm_arr: Array of shape (m, 1) where every element is sigmod of the corresponding element\n",
    "              in arr\n",
    "    sigmoid function (x) = 1 / (1 + exp(-x))\n",
    "    '''\n",
    "    m = arr.shape[0]\n",
    "    sigm_arr = 1 / (1 + np.exp(-arr))\n",
    "    sigm_arr = sigm_arr.reshape((m, 1))\n",
    "    return sigm_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:56.971042Z",
     "start_time": "2021-02-18T14:00:56.875542Z"
    }
   },
   "outputs": [],
   "source": [
    "def hypothesis_calc(theta, x):\n",
    "    '''\n",
    "    This function calculates hypothesis matrix.\n",
    "    \n",
    "    Arguements:\n",
    "    theta: Parameter matrix of shape (n, 1), n = number of features\n",
    "    x: Feature matrix of shape (m, n), m = number of training examples\n",
    "    \n",
    "    Returns:\n",
    "    h: Calculated hypothesis matrix of shape (m, 1)\n",
    "    '''\n",
    "    m = x.shape[0]\n",
    "    g = np.sum(np.multiply(np.transpose(theta), x), axis = 1)\n",
    "    g = g.reshape((m, 1))\n",
    "    h = sigmoid(g)\n",
    "    h = h.reshape((m, 1))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:57.093538Z",
     "start_time": "2021-02-18T14:00:56.975386Z"
    }
   },
   "outputs": [],
   "source": [
    "def cost_calc(h, y):\n",
    "    '''\n",
    "    This function calculates cost.\n",
    "    \n",
    "    Arguements:\n",
    "    h: Hypothesis matrix of shape (m, 1), m = number of training examples\n",
    "    y: Output matrix of shape (m, 1)\n",
    "    \n",
    "    Returns:\n",
    "    cost: Calculated cost based on hypothesis (h) and output matrix (y)\n",
    "    '''\n",
    "    epsilon = 1e-05\n",
    "    cost = -np.sum(np.multiply(y, np.log(h+epsilon)) + np.multiply((1-y), np.log(1-h+epsilon)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:57.208606Z",
     "start_time": "2021-02-18T14:00:57.098524Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_gradients(y, h, x, theta, learning_rate):\n",
    "    '''\n",
    "    This function updates parameter matrix using batch gradient descent algorithm.\n",
    "    \n",
    "    Arguements:\n",
    "    y: Output matrix of shape (m, 1), m = number of training examples\n",
    "    h: Hypothesis matrix of shape (m, 1)\n",
    "    x: Feature matrix of shape (m, n), m = number of training examples, n = number of features\n",
    "    theta: Parameter matrix of shape (n, 1)\n",
    "    learning_rate: Value of learning rate to be used to update parameter matrix\n",
    "    \n",
    "    Returns:\n",
    "    theta: Updated parameter matrix of shape (n, 1)\n",
    "    '''\n",
    "    n = theta.shape[0]\n",
    "    int_term = np.sum(np.multiply((y - h), x), axis = 0)\n",
    "    int_term = int_term.reshape((n, 1))\n",
    "    theta = theta + (learning_rate * int_term)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:57.310501Z",
     "start_time": "2021-02-18T14:00:57.213749Z"
    }
   },
   "outputs": [],
   "source": [
    "def convergence_check(costs, epsilon):\n",
    "    '''\n",
    "    This function checks convergence of gradient descent algorithm.\n",
    "    \n",
    "    Arguements:\n",
    "    costs: A list containing cost values of current and previous iterations\n",
    "    epsilon: Threshold of square error difference between costs of consecutive iterations used to\n",
    "    decide convergence of gradient descent algorithm\n",
    "    \n",
    "    Returns:\n",
    "    Boolean (True / False) value of whether algorithm has been converged\n",
    "    '''\n",
    "    error = (costs[0] - costs[1]) ** 2\n",
    "    return error < epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:57.439994Z",
     "start_time": "2021-02-18T14:00:57.318525Z"
    }
   },
   "outputs": [],
   "source": [
    "def logistic_regression(x, y, num_iterations = 50000, algo_type = 'batch', \n",
    "                                   learning_rate = 0.1, epsilon = 1e-04, verbose = True, initialization = 'zeros'):\n",
    "    '''\n",
    "    This function performs linear regression using gradient descent algorithm for minimising cost.\n",
    "    \n",
    "    Arguements:\n",
    "    x: Feature matrix of shape (m, n), m = number of training examples, n = number of features\n",
    "    y: Output matrix of shape (m, 1)\n",
    "    num_iterations (optional): Max number of iterations (default value: 50000) (if convergence is acheived before this number,\n",
    "                               algorithm will be stopped)\n",
    "    algo_type: 'batch' / 'stochastic' for batch / stochastic gradient descent algorithms.\n",
    "                Type of algorithm to be used for finding parameters\n",
    "    learning_rate (optional): Value for learning rate (default value: 0.1)\n",
    "    epsilon (optional): Threshold of square error difference between costs of consecutive iterations used to\n",
    "                        decide convergence of gradient descent algorithm (default value = 1e-04)\n",
    "    verbose (optional): Boolean value which decide whether the output of the algorithm will be verbose\n",
    "    initialization (optional): 'zeros' / 'random', parameter used for method of initialization of parameter matrix\n",
    "    \n",
    "    Returns:\n",
    "    theta: Parameter matrix of shape (n, 1)\n",
    "    costs: A dictionary with learning rate as key and list of costs for every 100th iteration as value\n",
    "    \n",
    "    Note: Ensure that dummy variable (x0) has been already added to the x matrix before passing through this function\n",
    "    '''\n",
    "    n = x.shape[1]\n",
    "    theta = initialize_parameter_matrix(n, method = initialization)\n",
    "    # print('Initial Parameters:')\n",
    "    h = hypothesis_calc(theta, x)\n",
    "    cost = cost_calc(h, y)\n",
    "    if verbose == True:\n",
    "        print('Cost:', cost)\n",
    "        # print('Parameter 1:', theta[0][0])\n",
    "        # print('Parameter 2:', theta[1][0])\n",
    "        print('*************************************')\n",
    "    costs = {}\n",
    "    costs_list = []\n",
    "    costs_list.append(cost)\n",
    "    if grad_desc_type == 'batch':\n",
    "        for i in range(num_iterations):\n",
    "            h = hypothesis_calc(theta, x)\n",
    "            theta = update_gradients(y, h, x, theta, learning_rate)\n",
    "            cost = cost_calc(h, y)\n",
    "            if verbose == True:\n",
    "                if ((i + 1) % 1000) == 0:\n",
    "                    print('Iteration:', i+1)\n",
    "                    print('Cost:', cost)\n",
    "                    #print('Parameter 1:', theta[0][0])\n",
    "                    #print('Parameter 2:', theta[1][0])\n",
    "                    print('*************************************')\n",
    "            if ((i + 1) % 100) == 0:\n",
    "                costs_list.append(cost)\n",
    "            if len(costs_list) >= 2:\n",
    "                if convergence_check(costs_list[-2:], epsilon):\n",
    "                    print('Alogorithm has converged')\n",
    "                    break\n",
    "        costs[learning_rate] = costs_list\n",
    "    elif grad_desc_type == 'stochastic':\n",
    "        for i in range(num_iterations):\n",
    "            index = i % (len(y) - 1)\n",
    "            for j in range(n):\n",
    "                theta[j][0] = theta[j][0] - learning_rate * (h[index] - y[index]) * x[index, :][j]\n",
    "            h = hypothesis_calc(theta, x)\n",
    "            cost = cost_calc(h, y)\n",
    "            if verbose == True:\n",
    "                if ((i + 1) % 1000) == 0:\n",
    "                    print('Iteration:', i+1)\n",
    "                    print('Cost:', cost)\n",
    "                    #print('Parameter 1:', theta[0][0])\n",
    "                    #print('Parameter 2:', theta[1][0])\n",
    "                    print('*************************************')\n",
    "            if ((i + 1) % 100) == 0:\n",
    "                costs_list.append(cost)\n",
    "            if len(costs_list) >= 2:\n",
    "                if convergence_check(costs_list[-2:], epsilon):\n",
    "                    print('Alogorithm has converged')\n",
    "                    break\n",
    "        costs[learning_rate] = costs_list\n",
    "    return theta, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:57.539250Z",
     "start_time": "2021-02-18T14:00:57.443757Z"
    }
   },
   "outputs": [],
   "source": [
    "def y_calc(theta, x):\n",
    "    '''\n",
    "    This function calculates output (y) using parameters (theta) and feature matrix.\n",
    "    \n",
    "    Arguements:\n",
    "    theta: Parameter matrix of shape (n, 1)\n",
    "    x: Feature matrix of shape (m, n), m = number of training / test set examples\n",
    "    \n",
    "    Returns:\n",
    "    y: Output / label matrix of shape (m, 1), the values of y will either be 1 or 0\n",
    "    \n",
    "    Note: Ensure that dummy variable (x0) has been already added to the x matrix before passing through this function\n",
    "    '''\n",
    "    h = hypothesis_calc(theta, x)\n",
    "    y = np.where(h > 0.5, 1, 0) \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:57.611974Z",
     "start_time": "2021-02-18T14:00:57.543441Z"
    }
   },
   "outputs": [],
   "source": [
    "def metrics_calc(y, y_pred):\n",
    "    '''\n",
    "    This function calculates metrics of binary classifier.\n",
    "    \n",
    "    Arguements:\n",
    "    y: Array of true labels\n",
    "    y_pred: Array of predicted labels\n",
    "    \n",
    "    Returns:\n",
    "    A tuple of accuracy, precision, recall and f1 score in percentages and round to 2 decimal digits\n",
    "    '''\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    acc_count = 0\n",
    "    for i, j in zip(y, y_pred):\n",
    "        if i == j:\n",
    "            acc_count = acc_count + 1\n",
    "        if i == 1 and j == 1:\n",
    "            tp = tp + 1\n",
    "        if i == 0 and j == 1:\n",
    "            fp = fp + 1\n",
    "        if i == 1 and j == 0:\n",
    "            fn = fn + 1\n",
    "    accuracy = acc_count / y.shape[0]\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return round(accuracy * 100, 2), round(precision * 100, 2), round(recall * 100, 2), round(f1 * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:57.725885Z",
     "start_time": "2021-02-18T14:00:57.614967Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using breast cancer dataset\n",
    "X, y = datasets.load_breast_cancer(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train, y_train = prepare_matrices(X_train_scaled, y_train)\n",
    "X_test, y_test = prepare_matrices(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:58.795785Z",
     "start_time": "2021-02-18T14:00:57.731073Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 295.27217900373563\n",
      "*************************************\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'grad_desc_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-17c9aa5823b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-dadd30f72a7b>\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(x, y, num_iterations, algo_type, learning_rate, epsilon, verbose, initialization)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mcosts_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mcosts_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mgrad_desc_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhypothesis_calc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grad_desc_type' is not defined"
     ]
    }
   ],
   "source": [
    "theta, costs = logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:58.809737Z",
     "start_time": "2021-02-18T14:00:44.925Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_pred = y_calc(theta, X_train)\n",
    "y_test_pred = y_calc(theta, X_test)\n",
    "train_accuracy, train_precision, train_recall, train_f1 = metrics_calc(y_train, y_train_pred)\n",
    "test_accuracy, test_precision, test_recall, test_f1 = metrics_calc(y_test, y_test_pred)\n",
    "print('Training Set Classification Metrics:')\n",
    "print('Train Accuracy: {}'.format(train_accuracy))\n",
    "print('Train Precision: {}'.format(train_precision))\n",
    "print('Train Recall: {}'.format(train_recall))\n",
    "print('Train F1 Score: {}'.format(train_f1))\n",
    "print('Test Set Classification Metrics:')\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n",
    "print('Test Precision: {}'.format(test_precision))\n",
    "print('Test Recall: {}'.format(test_recall))\n",
    "print('Test F1 Score: {}'.format(test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:58.813772Z",
     "start_time": "2021-02-18T14:00:44.930Z"
    }
   },
   "outputs": [],
   "source": [
    "# Used multiple learning rates to find optimal learning rate\n",
    "# The optimal learning rate for this example is 0.1 (with scaled data)\n",
    "'''\n",
    "theta_batch = []\n",
    "cost_batch = []\n",
    "lr_list_batch = [0.01, 0.02, 0.04, 0.1, 0.2]\n",
    "for i in lr_list_batch:\n",
    "    theta, cost = logistic_regression(X_train, y_train, learning_rate = i, algo_type = 'batch', verbose = False)\n",
    "    theta_batch.append(theta)\n",
    "    cost_batch.append(cost)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:58.816377Z",
     "start_time": "2021-02-18T14:00:44.935Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves for batch gradient descent with multiple learning rates\n",
    "'''\n",
    "plt.figure(figsize = (12, 8))\n",
    "for i in cost_batch:\n",
    "    plt.plot(list(i.values())[0], label = 'Learning Rate:{}'.format(list(i.keys())[0]))\n",
    "plt.xlabel('Number of Iterations (100s)')\n",
    "plt.ylabel('Cost')    \n",
    "plt.legend()\n",
    "plt.title('Learning Curves for Logistic Regression with Batch Gradient Descent');\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:58.818372Z",
     "start_time": "2021-02-18T14:00:44.939Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "theta_stochastic = []\n",
    "cost_stochastic = []\n",
    "lr_list_stoc = [0.01, 0.02, 0.04, 0.1, 0.2]\n",
    "for i in lr_list_stoc:\n",
    "    theta, cost = logistic_regression(X_train, y_train, learning_rate = i, algo_type = 'stochastic', verbose = False)\n",
    "    theta_stochastic.append(theta)\n",
    "    cost_stochastic.append(cost)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T14:00:58.820367Z",
     "start_time": "2021-02-18T14:00:44.945Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''# Plot the learning curves for stochastic gradient descent with multiple learning rates\n",
    "plt.figure(figsize = (12, 8))\n",
    "for i in cost_stochastic:\n",
    "    plt.plot(list(i.values())[0], label = 'Learning Rate:{}'.format(list(i.keys())[0]))\n",
    "plt.xlabel('Number of Iterations (100s)')\n",
    "plt.ylabel('Cost')    \n",
    "plt.legend()\n",
    "plt.title('Learning Curves for Logistic Regression with Stochastic Gradient Descent');\n",
    "''';"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
